# ============================================================================
# Multi-Provider LLM Configuration
# ============================================================================

# PHASE 1: Single LLM (Development)
# - Set only OPENAI_API_KEY for cost-effective testing
# - Use llm_mode='cheap' in create_ecosystem()

# PHASE 2: Multi-LLM Consensus (Production)
# - Set all three API keys below
# - Use llm_mode='multi' in create_ecosystem()
# - Enables Dawid-Skene consensus with 3 LLMs

# ----------------------------------------------------------------------------
# OpenAI API Configuration
# ----------------------------------------------------------------------------
# Get your API key from: https://platform.openai.com/api-keys

OPENAI_API_KEY=sk-your-api-key-here

# Optional: Override default model (default: gpt-3.5-turbo)
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo, o1, o1-mini
#OPENAI_MODEL=gpt-3.5-turbo

# ----------------------------------------------------------------------------
# Anthropic (Claude) API Configuration
# ----------------------------------------------------------------------------
# Get your API key from: https://console.anthropic.com/settings/keys

ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Optional: Override default model (default: claude-3-haiku-20240307)
# Options: claude-3-5-sonnet-20241022, claude-3-opus-20240229, claude-3-haiku-20240307
#ANTHROPIC_MODEL=claude-3-haiku-20240307

# ----------------------------------------------------------------------------
# Google (Gemini) API Configuration
# ----------------------------------------------------------------------------
# Get your API key from: https://makersuite.google.com/app/apikey

GOOGLE_API_KEY=AIza-your-api-key-here

# Optional: Override default model (default: gemini-2.5-flash)
# Options: gemini-2.5-flash, gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash
#GOOGLE_MODEL=gemini-2.5-flash

# ----------------------------------------------------------------------------
# Shared LLM Configuration
# ----------------------------------------------------------------------------

# Optional: Override default temperature (default: 0.7)
#LLM_TEMPERATURE=0.7

# Optional: Override max tokens (default: 2000)
#LLM_MAX_TOKENS=2000

# Optional: Override timeout (default: 30 seconds)
#LLM_TIMEOUT=30

# Optional: Cache configuration
#LLM_CACHE_ENABLED=true
#LLM_RETRY_ATTEMPTS=3
#LLM_RETRY_DELAY=2.0

# ----------------------------------------------------------------------------
# Multi-LLM Consensus Mode
# ----------------------------------------------------------------------------
# When using llm_mode='multi', the framework will:
# 1. Query all 3 LLMs (OpenAI, Claude, Gemini)
# 2. Apply Dawid-Skene algorithm for calibrated consensus
# 3. Detect and correct for judge bias
# 4. Provide reliability-weighted results
#
# Cost: ~3x single LLM ($6-15 per evaluation)
# Reliability: ~90-95% (vs ~80-85% single LLM)
