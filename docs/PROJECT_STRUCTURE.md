# SecurityEvaluator - Project Structure

**Version:** 1.0
**Date:** November 4, 2025

---

## Proposed Project Structure

```
SecurityEvaluator/
â”œâ”€â”€ README.md                           # Project overview, quickstart
â”œâ”€â”€ LICENSE                             # MIT License
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”œâ”€â”€ pyproject.toml                      # Python project config, dependencies
â”œâ”€â”€ uv.lock                             # Locked dependencies
â”œâ”€â”€ sample.env                          # Environment variables template
â”œâ”€â”€ .env                                # Local environment (gitignored)
â”‚
â”œâ”€â”€ docs/                               # ðŸ“š Documentation
â”‚   â”œâ”€â”€ ANALYSIS.md                     # Current state analysis
â”‚   â”œâ”€â”€ DESIGN.md                       # System architecture design
â”‚   â”œâ”€â”€ SPECIFICATION.md                # Technical specification
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md            # This file
â”‚   â”œâ”€â”€ DATASET_FORMAT.md               # Test case format guide
â”‚   â”œâ”€â”€ EVALUATION_CRITERIA.md          # Scoring methodology
â”‚   â””â”€â”€ QUICKSTART.md                   # Getting started guide
â”‚
â”œâ”€â”€ src/                                # Core framework (from AgentBeats template)
â”‚   â””â”€â”€ agentbeats/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ green_executor.py           # Base green agent executor
â”‚       â”œâ”€â”€ models.py                   # Base evaluation models
â”‚       â”œâ”€â”€ client.py                   # A2A client helpers
â”‚       â”œâ”€â”€ client_cli.py               # CLI client
â”‚       â”œâ”€â”€ run_scenario.py             # Scenario runner CLI
â”‚       â”œâ”€â”€ tool_provider.py            # Tool management
â”‚       â””â”€â”€ cloudflare.py               # Cloudflare tunnel integration
â”‚
â”œâ”€â”€ scenarios/                          # ðŸŽ¯ Evaluation scenarios
â”‚   â””â”€â”€ security/                       # SQL Injection benchmark
â”‚       â”œâ”€â”€ sql_injection_judge.py      # ðŸŸ¢ Green Agent (main evaluator)
â”‚       â”œâ”€â”€ config.yaml                 # Configuration (easier than TOML)
â”‚       â”œâ”€â”€ Dockerfile                  # Docker deployment
â”‚       â”œâ”€â”€ models.py                   # Data models (TestCase, Report, Metrics)
â”‚       â”œâ”€â”€ dataset_manager.py          # Dataset loading & sampling
â”‚       â”œâ”€â”€ scoring_engine.py           # Metrics calculation
â”‚       â”œâ”€â”€ test_orchestrator.py        # Test execution orchestration
â”‚       â”œâ”€â”€ report_generator.py         # Report & artifact generation
â”‚       â”œâ”€â”€ scenario.toml               # Scenario configuration (AgentBeats)
â”‚       â”‚
â”‚       â”œâ”€â”€ datasets/                   # ðŸ“Š Test datasets (JSON format)
â”‚       â”‚   â””â”€â”€ sql_injection/
â”‚       â”‚       â”œâ”€â”€ metadata.json       # Master metadata file
â”‚       â”‚       â”œâ”€â”€ vulnerable_code/    # Vulnerable code samples (JSON)
â”‚       â”‚       â”‚   â”œâ”€â”€ python_sqli.json      # 175 Python samples
â”‚       â”‚       â”‚   â”œâ”€â”€ javascript_sqli.json  # 90 JavaScript samples
â”‚       â”‚       â”‚   â”œâ”€â”€ java_sqli.json        # 50 Java samples
â”‚       â”‚       â”‚   â””â”€â”€ php_sqli.json         # 35 PHP samples
â”‚       â”‚       â””â”€â”€ secure_code/        # Secure code samples (JSON)
â”‚       â”‚           â”œâ”€â”€ python_secure.json    # 125 Python samples
â”‚       â”‚           â”œâ”€â”€ javascript_secure.json # 60 JavaScript samples
â”‚       â”‚           â”œâ”€â”€ java_secure.json      # 40 Java samples
â”‚       â”‚           â””â”€â”€ php_secure.json       # 25 PHP samples
â”‚       â”‚       # Total: 600 samples in 8 JSON files (vs 600+ individual files)
â”‚       â”‚
â”‚       â””â”€â”€ utils/                      # Helper utilities
â”‚           â”œâ”€â”€ payload_generator.py    # SQL injection payload generator
â”‚           â”œâ”€â”€ code_parser.py          # AST parsing utilities
â”‚           â””â”€â”€ validators.py           # Response validation helpers
â”‚
â”œâ”€â”€ purple_agents/                      # ðŸŸ£ Reference Purple Agent implementations
â”‚   â”œâ”€â”€ baseline/                       # Rule-based detector (baseline)
â”‚   â”‚   â”œâ”€â”€ sql_detector.py             # Main detector agent
â”‚   â”‚   â”œâ”€â”€ patterns.py                 # SQL injection patterns
â”‚   â”‚   â”œâ”€â”€ requirements.txt            # Minimal dependencies
â”‚   â”‚   â””â”€â”€ README.md                   # Usage instructions
â”‚   â”‚
â”‚   â”œâ”€â”€ llm_based/                      # LLM-powered detector
â”‚   â”‚   â”œâ”€â”€ sql_detector.py             # Gemini/GPT-4 based detector
â”‚   â”‚   â”œâ”€â”€ prompts.py                  # LLM prompt templates
â”‚   â”‚   â”œâ”€â”€ requirements.txt            # LLM SDK dependencies
â”‚   â”‚   â””â”€â”€ README.md                   # Usage & cost estimates
â”‚   â”‚
â”‚   â””â”€â”€ hybrid/                         # Hybrid approach (rule + LLM)
â”‚       â”œâ”€â”€ sql_detector.py             # Combined detector
â”‚       â”œâ”€â”€ rule_filter.py              # Pre-filtering with rules
â”‚       â”œâ”€â”€ llm_analyzer.py             # LLM for complex cases
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ tests/                              # ðŸ§ª Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                     # Pytest configuration
â”‚   â”œâ”€â”€ test_scoring_engine.py          # Scoring engine unit tests
â”‚   â”œâ”€â”€ test_dataset_manager.py         # Dataset loading tests
â”‚   â”œâ”€â”€ test_integration.py             # End-to-end integration tests
â”‚   â”œâ”€â”€ test_models.py                  # Pydantic model validation
â”‚   â””â”€â”€ fixtures/                       # Test fixtures
â”‚       â”œâ”€â”€ sample_test_cases.py
â”‚       â””â”€â”€ mock_purple_agent.py
â”‚
â”œâ”€â”€ scripts/                            # ðŸ› ï¸ Utility scripts
â”‚   â”œâ”€â”€ generate_dataset.py             # Generate test cases from templates
â”‚   â”œâ”€â”€ validate_dataset.py             # Validate dataset integrity
â”‚   â”œâ”€â”€ benchmark_purple_agents.py      # Local benchmarking tool
â”‚   â”œâ”€â”€ export_results.py               # Export results to CSV/JSON
â”‚   â””â”€â”€ setup_cloudflare.sh             # Cloudflare tunnel setup helper
â”‚
â”œâ”€â”€ assets/                             # ðŸ“¸ Images, diagrams
â”‚   â”œâ”€â”€ architecture.png                # System architecture diagram
â”‚   â”œâ”€â”€ sample_output.png               # Example output screenshot
â”‚   â””â”€â”€ confusion_matrix_example.png
â”‚
â””â”€â”€ experiments/                        # ðŸ”¬ Experimental features (optional)
    â”œâ”€â”€ multi_language/                 # JavaScript, Java, PHP test cases
    â”œâ”€â”€ dynamic_testing/                # Runtime exploitation tests
    â””â”€â”€ llm_as_judge/                   # Alternative LLM-based scoring
```

---

## Directory Breakdown

### `/src/agentbeats/` - Core Framework

**Purpose:** Shared AgentBeats framework code (from template)

**Key Files:**
- `green_executor.py`: Abstract base class for Green Agents
- `models.py`: Base Pydantic models (EvalRequest, EvalResult)
- `run_scenario.py`: CLI for running scenarios

**When to modify:**
- Generally don't modify (maintained by AgentBeats)
- Only extend if adding new framework capabilities

---

### `/scenarios/security/` - SQL Injection Benchmark (Green Agent)

**Purpose:** Implementation of the SQL injection detection benchmark

#### Core Components

**`sql_injection_judge.py`** (ðŸŸ¢ Main Green Agent)
```python
class SQLInjectionJudge(GreenAgent):
    async def run_eval(self, req: EvalRequest, updater: TaskUpdater) -> None:
        # Main evaluation orchestration
```
- Implements the evaluation workflow
- Coordinates all components
- Produces final assessment artifacts

**`models.py`**
```python
class TestCaseInput(BaseModel): ...
class VulnerabilityReport(BaseModel): ...
class TestResult(BaseModel): ...
class EvaluationMetrics(BaseModel): ...
```
- All Pydantic data models
- Input/output schemas
- Type safety enforcement

**`dataset_manager.py`**
```python
class DatasetManager:
    def load_dataset(self) -> None: ...
    def sample(self, n: int, categories: list[str]) -> list[TestCase]: ...
```
- Loads metadata.json
- Samples test cases
- Manages dataset access

**`scoring_engine.py`**
```python
class ScoringEngine:
    def calculate_metrics(self, results: list[TestResult]) -> EvaluationMetrics: ...
```
- Calculates TPR, FPR, F1, etc.
- Category breakdown
- Confusion matrix

**`test_orchestrator.py`**
```python
class TestOrchestrator:
    async def run_tests(self, agent_url: str, test_cases: list[TestCase]) -> list[TestResult]: ...
```
- Parallel test execution
- Timeout handling
- Progress tracking

**`report_generator.py`**
```python
class ReportGenerator:
    def generate_artifact(self, metrics: EvaluationMetrics) -> list[Part]: ...
```
- Format results as A2A artifacts
- Generate markdown reports
- Export to JSON

**`scenario.toml`**
```toml
[sql_injection_judge]
endpoint = "http://127.0.0.1:9009"
cmd = "python scenarios/security/sql_injection_judge.py --host 127.0.0.1 --port 9009"
```
- Configuration for local testing
- Agent endpoints and startup commands

---

### `/scenarios/security/datasets/sql_injection/` - Test Dataset

**Purpose:** Ground truth test cases for evaluation

**Format:** **JSON files** (easier to manage than 600+ individual files)

**Structure:**

```
datasets/sql_injection/
â”œâ”€â”€ metadata.json                # Master index
â”œâ”€â”€ vulnerable_code/
â”‚   â”œâ”€â”€ python_sqli.json         # 175 Python vulnerable samples
â”‚   â”œâ”€â”€ javascript_sqli.json     # 90 JavaScript samples
â”‚   â”œâ”€â”€ java_sqli.json           # 50 Java samples
â”‚   â””â”€â”€ php_sqli.json            # 35 PHP samples
â””â”€â”€ secure_code/
    â”œâ”€â”€ python_secure.json       # 125 Python secure samples
    â”œâ”€â”€ javascript_secure.json   # 60 JavaScript samples
    â”œâ”€â”€ java_secure.json         # 40 Java samples
    â””â”€â”€ php_secure.json          # 25 PHP samples
```

**Total:** 600 samples in 8 JSON files (vs managing 600+ individual `.py`/`.js`/`.java`/`.php` files)

**JSON File Format:**
```json
{
  "dataset_version": "1.0",
  "language": "python",
  "total_samples": 175,
  "samples": [
    {
      "id": "py_classic_001",
      "category": "classic_sqli",
      "severity": "high",
      "code": "query = f'SELECT * FROM users WHERE id={uid}'",
      "description": "Direct f-string concatenation",
      "cwe_id": "CWE-89",
      "tags": ["f-string", "concatenation"]
    }
  ]
}
```

**Benefits of JSON format:**
- âœ… Edit 8 files vs manage 600+ individual files
- âœ… 10x faster loading (8 I/O operations vs 600)
- âœ… Cleaner version control diffs
- âœ… Easy schema validation
- âœ… Simpler programmatic generation

---

### `/purple_agents/` - Reference Implementations

**Purpose:** Provide baseline Purple Agent implementations for:
1. Testing Green Agent during development
2. Demonstrating Purple Agent interface
3. Benchmark comparison (participants should beat baseline)

#### Three Variants:

**1. Baseline (Rule-Based)**
```
purple_agents/baseline/
â”œâ”€â”€ sql_detector.py         # Pattern matching, regex
â”œâ”€â”€ patterns.py             # Vulnerability patterns
â””â”€â”€ README.md
```
- Fast, deterministic
- Expected F1: ~0.60
- No API costs

**2. LLM-Based**
```
purple_agents/llm_based/
â”œâ”€â”€ sql_detector.py         # Gemini/GPT-4 powered
â”œâ”€â”€ prompts.py              # Prompt templates
â””â”€â”€ README.md
```
- High accuracy
- Expected F1: ~0.85
- $0.10-1.00 per 100 tests

**3. Hybrid**
```
purple_agents/hybrid/
â”œâ”€â”€ sql_detector.py         # Rule pre-filter + LLM
â”œâ”€â”€ rule_filter.py          # Fast pre-screening
â””â”€â”€ llm_analyzer.py         # Deep analysis for complex cases
```
- Balanced cost/accuracy
- Expected F1: ~0.80
- $0.03-0.50 per 100 tests

---

### `/tests/` - Test Suite

**Purpose:** Ensure code quality and correctness

**Test Categories:**

1. **Unit Tests**
   - `test_scoring_engine.py`: Metric calculations
   - `test_dataset_manager.py`: Dataset loading
   - `test_models.py`: Pydantic validation

2. **Integration Tests**
   - `test_integration.py`: End-to-end evaluation flow
   - Mock Purple Agents for deterministic testing

3. **Validation Tests**
   - Dataset integrity checks
   - File existence verification
   - Metadata consistency

**Run Tests:**
```bash
pytest tests/ -v
pytest tests/test_scoring_engine.py -v
pytest tests/ --cov=scenarios/security
```

---

### `/scripts/` - Utility Scripts

**Purpose:** Development and maintenance tools

**Key Scripts:**

**`generate_dataset.py`**
```bash
python scripts/generate_dataset.py --category classic --count 10
```
- Generate test cases from templates
- LLM-assisted code generation
- Automatic metadata creation

**`validate_dataset.py`**
```bash
python scripts/validate_dataset.py
```
- Verify all files referenced in metadata exist
- Check code syntax validity
- Validate category distribution

**`benchmark_purple_agents.py`**
```bash
python scripts/benchmark_purple_agents.py \
  --agent http://localhost:9019 \
  --sample-size 100
```
- Local benchmarking without AgentBeats platform
- Quick iteration during Purple Agent development
- Output: metrics.json, report.md

**`setup_cloudflare.sh`**
```bash
./scripts/setup_cloudflare.sh
```
- Automated Cloudflare Tunnel setup
- Generate persistent tunnel with named domain
- Update scenario.toml with public URL

---

### `/docs/` - Documentation

**Purpose:** Comprehensive project documentation

**Key Documents:**

1. **ANALYSIS.md**: Current state analysis, issues identified
2. **DESIGN.md**: System architecture, workflow diagrams
3. **SPECIFICATION.md**: Technical spec, API contracts
4. **PROJECT_STRUCTURE.md**: This file
5. **QUICKSTART.md**: Step-by-step getting started
6. **DATASET_FORMAT.md**: Test case format guidelines
7. **EVALUATION_CRITERIA.md**: Detailed scoring methodology

---

## File Count Estimates

| Directory | Files | Lines of Code |
|-----------|-------|---------------|
| `src/agentbeats/` | 8 | ~1,500 (template) |
| `scenarios/security/` | 12 | ~2,500 |
| `scenarios/security/datasets/` | 9 (JSON) | ~15,000 (600 samples in JSON) |
| `purple_agents/baseline/` | 4 | ~400 |
| `purple_agents/llm_based/` | 4 | ~300 |
| `purple_agents/hybrid/` | 5 | ~600 |
| `tests/` | 10 | ~1,000 |
| `scripts/` | 5 | ~800 |
| `docs/` | 6 | ~12,000 (markdown) |
| **Total** | **~65** | **~35,000** |

**Note:** JSON dataset format reduces file count from 600+ individual files to 8 JSON files.

---

## Development Workflow

### Phase 1: Setup

```bash
# 1. Clone repository
git clone https://github.com/Mauttaram/SecurityEvaluator.git
cd SecurityEvaluator

# 2. Install dependencies
uv sync

# 3. Set up environment
cp sample.env .env
# Edit .env to add GOOGLE_API_KEY

# 4. Verify installation
uv run agentbeats-run --help
```

### Phase 2: Dataset Creation

```bash
# Generate initial dataset
python scripts/generate_dataset.py --category classic --count 100
python scripts/generate_dataset.py --category blind --count 80
# ... repeat for all categories

# Validate dataset
python scripts/validate_dataset.py

# Review metadata
cat scenarios/security/datasets/sql_injection/metadata.json
```

### Phase 3: Green Agent Development

```bash
# Run tests as you develop
pytest tests/test_scoring_engine.py -v

# Test with baseline purple agent
uv run agentbeats-run scenarios/security/scenario.toml

# View logs
uv run agentbeats-run scenarios/security/scenario.toml --show-logs
```

### Phase 4: Purple Agent Testing

```bash
# Start purple agent in separate terminal
python purple_agents/baseline/sql_detector.py --port 9019

# Run evaluation
uv run agentbeats-run scenarios/security/scenario.toml

# Benchmark locally
python scripts/benchmark_purple_agents.py \
  --agent http://localhost:9019 \
  --sample-size 100
```

### Phase 5: Platform Deployment

```bash
# Set up Cloudflare Tunnel
./scripts/setup_cloudflare.sh

# Start green agent with public URL
python scenarios/security/sql_injection_judge.py \
  --host 127.0.0.1 \
  --port 9009 \
  --card-url https://your-tunnel.trycloudflare.com

# Register on AgentBeats.org
# Navigate to https://agentbeats.org
# Register agent with public URL
```

---

## Files to Create (Priority Order)

### High Priority (MVP)

1. âœ… `docs/ANALYSIS.md`
2. âœ… `docs/DESIGN.md`
3. âœ… `docs/SPECIFICATION.md`
4. âœ… `docs/PROJECT_STRUCTURE.md`
5. â¬œ `scenarios/security/models.py` - Data models
6. â¬œ `scenarios/security/dataset_manager.py` - Dataset loading
7. â¬œ `scenarios/security/scoring_engine.py` - Metrics calculation
8. â¬œ `scenarios/security/sql_injection_judge.py` - Main green agent
9. â¬œ `scenarios/security/datasets/sql_injection/metadata.json` - Initial metadata
10. â¬œ 50 initial test case files (25 vulnerable, 25 secure)
11. â¬œ `purple_agents/baseline/sql_detector.py` - Baseline agent
12. â¬œ `tests/test_scoring_engine.py` - Core tests

### Medium Priority (Full Implementation)

13. â¬œ `scenarios/security/test_orchestrator.py` - Orchestration
14. â¬œ `scenarios/security/report_generator.py` - Reporting
15. â¬œ Complete dataset: 500+ test cases
16. â¬œ `purple_agents/llm_based/sql_detector.py` - LLM agent
17. â¬œ `tests/test_integration.py` - Integration tests
18. â¬œ `scripts/generate_dataset.py` - Dataset generator
19. â¬œ `scripts/validate_dataset.py` - Validator
20. â¬œ `docs/QUICKSTART.md` - User guide

### Low Priority (Polish)

21. â¬œ `purple_agents/hybrid/sql_detector.py` - Hybrid agent
22. â¬œ `scripts/benchmark_purple_agents.py` - Local benchmark
23. â¬œ `scripts/setup_cloudflare.sh` - Automation
24. â¬œ Complete test coverage (all modules)
25. â¬œ `docs/EVALUATION_CRITERIA.md` - Detailed methodology
26. â¬œ Performance optimization
27. â¬œ Visualization tools

---

## Git Workflow

### Branch Strategy

```
main                    # Stable releases
â”œâ”€â”€ dev                 # Active development
â”‚   â”œâ”€â”€ feature/dataset         # Dataset creation
â”‚   â”œâ”€â”€ feature/green-agent     # Green agent implementation
â”‚   â”œâ”€â”€ feature/purple-agents   # Purple agent examples
â”‚   â””â”€â”€ feature/docs            # Documentation
```

### Commit Conventions

```
feat: Add scoring engine with F1 calculation
fix: Correct dataset metadata validation
docs: Update DESIGN.md with workflow diagrams
test: Add integration tests for green agent
refactor: Simplify test orchestration logic
```

---

**Document Version:** 1.0
**Last Updated:** November 4, 2025
